---
title: 'Ollama: Running Large Language Models Locally'
excerpt: "We have reached a point where the barrier to entry for using Large
Language models (LLMs) is becoming so low. That almost anyone who wants to experiement,
can. This is one of the most beautiful aspects of Ollama. You can download a
program on most major operating systems (OS). Run a command in the command
line/terminal and be able to chat with a whole list of different models. It's
incredible!"
coverImage: '/assets/blog/img_bin/ollama_logo.png'
date: '2024-02-21T13:50:52.412Z'
author:
  name: 'Justin Bender'
  picture: '/assets/blog/authors/bender.png'
ogImage:
  url: '/assets/blog/img_bin/ollama_logo.png'
---

## Ollama: Running Large Language Models locally 

> We have reached a point where the barrier to entry for using Large
Language models (LLMs) is becoming so low. That almost anyone who wants to experiement,
can. This is one of the most beautiful aspects of Ollama. You can download a
program on most major operating systems (OS). Run a command in the command
line/terminal and be able to chat with a whole list of different models. It's
incredible!

### Getting up and running with Llama 2, Mistral and other large language models

To find the best source of information I recommend starting on the [Ollama
Github](https://github.com/ollama/ollama) page. There is some information that
is very useful that can be found on this page.

* You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.



    
